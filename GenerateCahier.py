import os
import re
import time
import hashlib
import pdfplumber
import ollama
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import subprocess




CACHE_DIR = "cache"


def extract_text_from_pdf(pdf_path):
    """
    Extrait tout le texte dâ€™un fichier PDF (y compris les tableaux).
    """
    all_text = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text:
                all_text.append(text)
            tables = page.extract_tables()
            for table in tables:
                table_text = "\n".join([
                    "\t".join(cell if cell is not None else "" for cell in row)
                    for row in table if row
                ])
                all_text.append(table_text)
    return "\n\n".join(all_text)

"""def split_into_sentences(text):

    sentence_endings = re.compile(r'(?<=[.!?]) +')
    sentences = sentence_endings.split(text)
    return sentences"""

def chunk_text(text, max_size=2500):
    """
    DÃ©coupe le texte en morceaux ("chunks") de taille maximale max_size,
    sans dÃ©couper au milieu des phrases (simplifiÃ©, dÃ©coupe juste par paragraphe ou morceaux fixes).
    """
    paragraphs = text.split("\n\n")
    chunks = []
    current = ""

    for p in paragraphs:
        if len(p) <= max_size:
            # Si le paragraphe est court, on tente de l'ajouter au chunk courant
            if len(current) + len(p) + 2 <= max_size:
                current += p + "\n\n"
            else:
                if current:
                    chunks.append(current.strip())
                current = p + "\n\n"
        else:
            # Paragraphe trop long, on coupe en morceaux fixes max_size
            for i in range(0, len(p), max_size):
                piece = p[i:i+max_size]
                if len(current) + len(piece) + 1 <= max_size:
                    current += piece + " "
                else:
                    if current:
                        chunks.append(current.strip())
                    current = piece + " "

            current += "\n\n"

    if current:
        chunks.append(current.strip())

    print(f"Chunks count after resizing: {len(chunks)}")
    return chunks

def ensure_cache_dir():
    if not os.path.exists(CACHE_DIR):
        os.makedirs(CACHE_DIR)

def get_cache_filename(text_chunk):
    """
    GÃ©nÃ¨re un nom de fichier unique pour chaque chunk Ã  partir de son hash MD5.
    Permet de stocker en cache les rÃ©ponses LLM dans le dossier cache/.
    """
    chunk_hash = hashlib.md5(text_chunk.encode('utf-8')).hexdigest()
    return os.path.join(CACHE_DIR, f"cache_{chunk_hash}.txt")  # <-- Add cache/ folder here

def save_to_cache(text_chunk, response):
    """
    Sauvegarde la rÃ©ponse dans un fichier cache spÃ©cifique au chunk.
    """
    ensure_cache_dir()  # Make sure cache/ exists before saving

    filename = get_cache_filename(text_chunk)
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(response)

def load_from_cache(text_chunk):
    """
    Charge la rÃ©ponse d'un chunk depuis le cache si disponible.
    """
    filename = get_cache_filename(text_chunk)
    if os.path.exists(filename):
        with open(filename, 'r', encoding='utf-8') as f:
            return f.read()
    return None

def traiter_chunk_avec_cache(text_chunk):
    """
    Traite un chunk :
    - VÃ©rifie le cache avant d'appeler Ollama
    - Envoie le prompt Ã  Ollama avec le chunk
    - Sauvegarde la rÃ©ponse en cach
    """
    cached = load_from_cache(text_chunk)
    if cached is not None:
        print("â™»ï¸ RÃ©ponse chargÃ©e depuis le cache.")
        return cached

    start = time.time()
    print(f"â±ï¸ DÃ©marrage Ã  {datetime.now().strftime('%H:%M:%S.%f')[:-3]}")

    prompt = f"""
Tu es un expert en analyse de cahiers des charges techniques. Ã€ partir du texte ci-dessous, ta tÃ¢che est dâ€™extraire **uniquement** les exigences fonctionnelles et non fonctionnelles, et de les formuler de maniÃ¨re **trÃ¨s concise**, en te limitant Ã  **6 mots maximum**, avec un vocabulaire simple et clair.

---

### Instructions :
- Ignore les exigences sans action claire, objet ou contexte prÃ©cis, ainsi que les dÃ©tails inutiles pour un scÃ©nario de test.
- Ne conserve que les exigences dÃ©crivant une interaction observable par un utilisateur final, avec assez dâ€™informations pour Ãªtre testÃ©es.
- DÃ©cris uniquement ce que lâ€™utilisateur (rÃ´le) peut faire, voir ou recevoir depuis lâ€™extÃ©rieur du systÃ¨me.
- Chaque exigence doit inclure un **rÃ´le** (ex : utilisateur, Ã©tudiant), un **verbe** (action), et un **objet**.  
- Lâ€™**objet** peut Ãªtre dÃ©taillÃ© si nÃ©cessaire pour Ãªtre clair et comprÃ©hensible.
- Utilise un vocabulaire standardisÃ© et simple (ex : Â« Ã‰tudiant s'inscrire Â», Â« Utilisateur voir messages Â»).
- Reformule toute exigence avec plusieurs actions en exigences sÃ©parÃ©es avec une seule action chacune.
- Supprime les doublons ou exigences trÃ¨s similaires.
- Ignore les rÃ´les techniques (administrateur, backend, base de donnÃ©es).
- Ne gÃ©nÃ¨re **aucun texte explicatif**. Seulement deux listes.

---

### Format de sortie attendu :

Exigences Fonctionnelles :  
1. ...  
2. ...  
3. ...  



Voici le texte Ã  analyser :
{text_chunk}
"""









    response = ollama.generate(
        model="llama3.1",
        prompt=prompt,
        options={"temperature": 0}
    )
    end = time.time()
    print(f"âœ… TerminÃ© Ã  {datetime.now().strftime('%H:%M:%S.%f')[:-3]} â€” durÃ©e : {end - start:.2f}s")

    result = response['response'].strip()
    save_to_cache(text_chunk, result)
    return result



def organize_exigences(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    func_pattern = re.compile(
        r'Exigences Fonctionnelles\s*:(.*?)(?=Exigences Fonctionnelles|Exigences Non Fonctionnelles|$)',
        re.DOTALL
    )
    func_blocks = func_pattern.findall(content)

    def extract_items(block):
        # Extraction simple des lignes numÃ©rotÃ©es
        items = re.findall(r'\d+\.\s+(.*)', block)
        return [item.strip().replace('\n', ' ') for item in items if item.strip()]

    all_func = []
    for block in func_blocks:
        all_func.extend(extract_items(block))

    output = "Exigences Fonctionnelles :\n"
    for i, req in enumerate(all_func, 1):
        output += f"{i}. {req}\n"

    return output


def main():
    # Extraction texte complet du PDF
    raw_cahier = extract_text_from_pdf("cahier1.pdf")

    # DÃ©coupage du texte en morceaux
    chunks = chunk_text(raw_cahier)
    print(f"âœ… Cahier divisÃ© en {len(chunks)} morceaux.\n")

    all_outputs = [""] * len(chunks)

    with ThreadPoolExecutor(max_workers=2) as executor:
        futures = {executor.submit(traiter_chunk_avec_cache, chunk): idx for idx, chunk in enumerate(chunks)}
        for future in as_completed(futures):
            idx = futures[future]
            try:
                result = future.result()
                all_outputs[idx] = result
            except Exception as e:
                print(f"âš ï¸ Erreur lors du traitement du chunk {idx}: {e}")

    # Fusion des rÃ©sultats bruts
    full_result = "\n\n".join(all_outputs)
    with open("exigences.txt", "w", encoding="utf-8") as f:
        f.write(full_result)
    print("ðŸ“„ Exigences brutes enregistrÃ©es dans exigences.txt")

    # Nettoyage final
    organized_text = organize_exigences('exigences.txt')

   

    # Optional: save to a new file
    with open('exigences_organisees.txt', 'w', encoding='utf-8') as f_out:
        f_out.write(organized_text)


    subprocess.run(['python', 'Transform.py'], check=True)
    ##subprocess.run(['python', 'verifier_exigences.py'], check=True)


if __name__ == "__main__":
    main()